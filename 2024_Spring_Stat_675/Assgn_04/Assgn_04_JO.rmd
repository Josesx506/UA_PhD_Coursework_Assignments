---
title: "Assignment_04_JO"
output:
  pdf_document: default
---

```{r}
library(ggplot2)
set.seed(808)
# knitr::opts_chunk$set(fig.width = 8, fig.height = 12)
```

1. Consider a density function $f(x) = 1 - |x| for -1 \le x \le 1$.
    (a) Create a function which can generate n IID random variables from the density f(x) by inverse
    transformation method.
    (b) Create a function which can generate n IID random variables from the density f(x) by acceptance rejection
    method.
    (c) If X and Y independently follow standard uniform distribution, then Z = X - Y follows a
    distribution with density f(x). Given this fact, create a function which can generate n IID
    random variables from the density f(x) by direct transformation method.
    (d) Draw histograms to check your random number generators. Do your histograms look correct?

```{r}
# (a) Inverse Transformation Method
inverse_transform <- function(n) {
  u <- runif(n)
  x <- ifelse(u <= 0.5, sqrt(2 * u) - 1, 1 - sqrt(2 * (1 - u)))
  return(x)
}
# Generate n random variables
n <- 1000
rv_it <- inverse_transform(n)



# (b) Acceptance-Rejection Method
random_acceptance_rejection <- function(n) {
  c <- 2  # Upper bound on f(x)/g(x)
  x <- numeric(n)
  i <- 1
  while (i <= n) {
    u1 <- runif(1)
    u2 <- runif(1)
    x_candidate <- -1 + 2 * u1
    if (u2 <= (1 - abs(x_candidate)) / (c * 0.5)) {
      x[i] <- x_candidate
      i <- i + 1
    }
  }
  return(x)
}
# Generate n random variables
rv_ar <- random_acceptance_rejection(n)



# (c) Direct Transformation Method
random_direct_transformation <- function(n) {
  x <- runif(n)
  y <- runif(n)
  z <- x - y
  return(z)
}
# Generate n random variables
rv_dt <- random_direct_transformation(n)

# (d) Plot and compare all the histograms
par(mfrow = c(2, 2))

y <- seq(-1, 1, .01)

hist(rv_it, breaks = 30, xlab = "x",
  freq = FALSE, main = "Inverse Transformation Method",
  col = "lightblue", xlim = c(-1, 1)
)
lines(y, 1 - abs(y)) # Density  curve

hist(rv_ar, freq = FALSE, breaks = 30, xlab = "x",
     main = "Acceptance-Rejection Method", col = "lightgreen", xlim = c(-1, 1))
lines(y, 1 - abs(y)) # Density  curve

hist(rv_dt, freq = FALSE, breaks = 30, xlab = "x",
     main = "Direct Transformation Method", col = "lightcoral", xlim = c(-1, 1))
lines(y, 1 - abs(y)) # Density  curve

par(mfrow = c(1, 1))  # Reset plotting parameters
```

2. Let $X_1,..., X_m$ be a random sample from normal distribution with mean $\mu$ and variance $\sigma^2$. 
Let $\overline X_m = \frac{1} {n} \displaystyle\sum_{i=1}^{m} X_i$ be the sample mean, and 
$S_{m}^{2} = \frac{1} {m-1} \displaystyle\sum_{i=1}^{m} (X_i - \overline X)^2$ be sample variance.
    (a) Probability theory tells us that $T = \frac{\overline X_m - \mu} {\sqrt{\frac{S_{m}^{2}} {m}}}$
    follows t distribution with degrees of freedom m - 1. Use this fact to write a function to generate random 
    variables with t distribution: input a sample size n and degrees of freedom d, output n IID random numbers 
    from t distribution with degrees of freedom d.
    (b) Let n = 1000 and d = 10. Draw 1000 random numbers using (a) and compare the histogram
    with the true density of t distribution. (check function dt for density of t distribution.)
    (c) Plot the empirical CDF of 1000 random numbers in (b), and compare it with the true CDF.

```{r}
# (a)
t_distribution_generator <- function(n, df) {
  # Generate normal samples
  z <- rnorm(n)
  # Calculate Chi-squared samples
  chi2 <- rchisq(n, df = df)
  # Apply the formula
  t <- z / sqrt(chi2 / df)
  return(t)
}

# (b)
n <- 1000
df <- 10

# Generate samples and true density
samples <- t_distribution_generator(n, df)
x <- seq(-5, 5, length = n)
theoretical_density <- dt(x, df = df)

# Plot histogram and density
hist(samples, main = "T-distribution samples", breaks = 20,
     freq = FALSE, ylim = c(0, 0.4))
lines(x, theoretical_density, col = "red", lwd = 2)

# (c)
# Calculate empirical CDF
ecdf <- ecdf(samples)

# Plot empirical and true CDF
plot(ecdf, main = "Empirical CDF vs. True CDF", xlab = "x", ylab = "P(X <= x)")
lines(x, pt(x, df = df), col = "red", lwd = 2)
```

3. Here are three ways to generate standard normal N(0, 1) random variables from standard uniform
distribution U(0, 1).
    (I) Box-Muller Algorithm.
        (i) Generate $U_1, U_2 \stackrel{\text{IID}}{\sim} U(0, 1)$
        (ii) Define $X_1 = \sqrt{-2 log U_1} cos(2 \pi U_2)$ and $X_2 = \sqrt{2 log U_1} sin(2 \pi U_2)$.
        Then $X_1, X_2 \stackrel{\text{IID}}{\sim} N(0, 1)$.
    (II)  Acceptance-Rejection Algorithm.
        (i) Generate random variables from standard Cauchy distribution by inverse transformation method.
        (ii) Generate standard normal variables from Cauchy distribution by Acceptance-Rejection method.
    (III) Acceptance-Rejection Algorithm 2.
        (i) Generate random variables from double-exponential distribution by inverse transformation method.
        (ii) Generate standard normal variables from double-exponential distribution by Acceptance-Rejection method.
    
    (a) Write a functions to implement Algorithm I with input n, and output x, which is a vector of n
    IID standard normal random variables.
    (b) For each of Algorithms II and III, find constant c, such that the acceptance-rejection algorithm gets
    maximal acceptance probability. Find the mean (average) number of trials in order to generate
    one standard normal variable. Implement the algorithm and create a function with input n, and
    output x, a vector of n IID standard normal random variables.
    (c) Draw histograms to check your random number generators in (a) and (b). Do your histograms
    look correct?

```{r fig.height = 8, fig.width = 6}
# (a) Box-Muller
box_muller_algo <- function(n) {
  u1 <- runif(n)
  u2 <- runif(n)
  x1 <- sqrt(-2 * log(u1)) * cos(2 * pi * u2)
  x2 <- sqrt(-2 * log(u1)) * sin(2 * pi * u2)
  return(c(x1, x2))
}



# (b)
# Cauchy distribution inverse-transform method
inverse_cauchy <- function(n, sigma = 1, mu = 0) {
  u <- runif(n)
  ca <- sigma * tan(pi * (u - (1 / 2))) + mu
  return(ca)
}
# Acceptance-rejection Cauchy distribution
# ar_cauchy <- function(n) {
#   result <- numeric(n)
#   i <- 1                 # Succesful counts
#   trials <- 1
#   while (i <= n) {
#     u1 <- runif(1)
#     u2 <- runif(1)
#     x_can <- tan(pi * (u1 - 0.5))

#     if (u2 <= sqrt(2 / pi) * exp(-0.5 * x_can^2) / (1 + x_can^2)) {
#       result[i] <- x_can
#       i <- i + 1
#     }

#     trials <- trials + 1
#   }
#   return(list(mean_trials = mean(trials), n_trials = trials,
#               n_var = result))
# }

ar_cauchy <- function(n) {
  trials <- numeric(n)
  result <- numeric(n)

  for (i in 1:n) {
    accept <- FALSE
    while (!accept) {
      u1 <- runif(1)
      u2 <- runif(1)

      x <- tan(pi * (u1 - 0.5))

      if (u2 <= sqrt(2 / pi) * exp(-0.5 * x^2) / (1 + x^2)) {
        result[i] <- x
        accept <- TRUE
      } else {
        trials[i] <- trials[i] + 1
      }
    }
  }

  return(list(mean_trials = mean(trials), n_trials = trials,
              n_var = result))
}

# (d) Histograms
n <- 1000
x_bm <- box_muller_algo(n)
x_cauchy_it <- inverse_cauchy(n)
x_cauchy_ar <- ar_cauchy(n)
cat("The mean number of trials is", x_cauchy_ar$mean_trials, ".\n")

par(mfrow = c(3, 1))  # Plot multiple histograms
hist(x_bm, main = "Box-Muller", breaks = 20)

hist(x_cauchy_it, main = "Cauchy Inverse Transform", breaks = 20,
     freq = FALSE)
# hist(x_cauchy_ar$n_var, main = "Cauchy Acceptance Rejection", breaks = 20,
#      freq = FALSE)

# Add the true density curve of the Cauchy distribution
x_vals <- seq(-2, 2, length.out = n)
true_density_cauchy <- dcauchy(x_vals)
true_density_cauchy1 <- 1 / (pi * (1 + x_vals^2))
lines(x_vals, true_density_cauchy, col = "red", lwd = 2)
lines(x_vals, true_density_cauchy1, col = "#52986c", lwd = 2)

par(mfrow = c(1, 1))  # Reset plotting parameters
```

4. The unit sphere $S^2$ in Euclidean space $\mathbb{R}^3$ is a set defined by equation $x^2_1 + x^2_2 + x^2_3 = 1$.
    (a) Follow the steps below to create a function to generate n IID random vector uniformly distributed
    on the unit sphere S2. (i) To generate one random vector uniformly distributed in the unit sphere
    $S^2$, start with 3 IID random numbers $Z_1, Z_2, Z_3$ from standard normal distribution. Define
    $X_i = Z_i / \sqrt{Z^2_1 + Z^2_2 + Z^2_3}$ for each i. Then $(X_1,X_2,X_3)$ is a random vector uniformly distributed
    in $S^2$. (ii) By a loop or matrix operations, you can generate n random vectors.
    (b) For two random vectors uniformly distributed in $S^2$, what is the expected distance between them?
    Do not calculate it theoretically. Do a simulation to estimate the expected distance. What is the
    standard error of your estimate?
    (c) In part (a), if you generate $Z_1, Z_2, Z_3$ from uniform distribution over the interval $[-1, 1]$ instead of
    the standard normal distribution, and define $(X_1,X_2,X_3)$ the same way, you can also get random
    vectors on the unit sphere $S^2$. Give some numerical evidence.

```{r}
```

5. (a) Write a function with input n and d and output n IID random vector uniformly distributed within
    d-dimensional unit ball (with radius 1).
    (b) For two random vectors in 3D unit ball, what is the expected distance between them? Do not
    calculate it theoretically. Do a simulation to estimate the expected distance.
    (c) Let $\rho_d$ be the ratio between the volume of a unit ball and the volume of a cube with side 2, in
    Rd. Estimate $\rho_d$ for $d = 2, ..., 5$.
    (d) Search online and find the exact values of $\rho_d$. Compare it with your estimate.

```{r}
```