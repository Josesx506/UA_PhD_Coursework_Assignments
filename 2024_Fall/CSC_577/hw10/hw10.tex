%
% Latex comments start with the percent symbol.
%
% This file should create a pdf on a mac or Linux command line by running:
%     pdflatex hw10.tex
% I usually add a few options
%     pdflatex -halt-on-error -interaction=nonstopmode -file-line-error hw10.tex
% 
% If you are new to Latex, you might not know that you may need to run the above
% twice for the compiler to sort out its references. (There are ways to finesse
% this). 
%

\documentclass[12pt]{report}

% Whether or not you need all these packages, or even some more will vary. These
% are some common ones, but not all are needed for this document. There is no
% real harm loading your favorites out of habit. 


\usepackage{algorithm,algorithmic,alltt,amsmath,amssymb,bm,
    cancel,color,fullpage,graphicx,listings,mathrsfs,
    multirow,setspace,subcaption,upgreek,xcolor}
\usepackage[numbered,framed]{matlab-prettifier}
\usepackage[colorlinks]{hyperref}
\usepackage[nameinlink,noabbrev]{cleveref}
\usepackage[verbose]{placeins}
\usepackage{caption}
\usepackage[skip=0.1ex, belowskip=1ex,
            labelformat=brace,
            singlelinecheck=off]{subcaption}
\usepackage{float, wrapfig, multicol}
\usepackage[backend=bibtex,style=alphabetic]{biblatex}
\bibliography{sample.bib}

\doublespacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%% Operators %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Your personal shortcuts. You do not need to use any. argmax and argmin
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%% Distributions
\newcommand{\N}{\mathcal{N}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\Poi}{{\text Poisson}}
\newcommand{\Exp}{{\text Exp}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\Ber}{{\text Bern}}
\newcommand{\Lap}{{\text Laplace}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
% \usepackage[left=1cm,right=2.5cm,top=2cm,bottom=1.5cm]{geometry}
% Code blocks formatting

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0}

% For faster processing, load Matlab syntax for listings
\lstloadlanguages{Matlab}%
\lstset{language=Matlab,        % Use MATLAB
        % frame=single,   % Single frame around code
        basicstyle=\small\ttfamily,     % Use small true type font
        keywordstyle=[1]\color{blue}\bfseries,  % MATLAB functions bold and blue
        keywordstyle=[2]\color{purple}, % MATLAB function arguments purple
        keywordstyle=[3]\color{blue}\underbar,  % User functions underlined and blue
        identifierstyle=,       % Nothing special about identifiers
        % Comments small dark green courier
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small,
        stringstyle=\color{purple},     % Strings are purple
        showstringspaces=false,         % Don't put marks in string spaces
        tabsize=2,      % 2 spaces per tab
        %%% Put standard MATLAB functions not included in the default language here
        morekeywords={xlim,ylim,var,alpha,factorial,poissrnd,normpdf,normcdf},
        %%% Put MATLAB function parameters here
        morekeywords=[2]{on, off, interp},
        %%% Put user defined functions here
        morekeywords=[3]{hw1,hw2,},
        gobble=4,
        morecomment=[l][\color{blue}]{...},     % Line continuation (...) like blue comment
        numbers=left,   % Line numbers on left
        firstnumber=1,          % Line numbers start with line 1
        numberstyle=\tiny\color{blue},  % Line numbers are blue
        stepnumber=5    % Line numbers go in steps of 5
}

%% Probability
\newcommand{\E}[1]{\mathbb{E}[#1]}
\newcommand{\Cov}[2]{\mathbb{C}\mathrm{ov}(#1,#2)}

%% Bold font for vectors from Ernesto, but I do not know how the first one
%  works, but it seems necessary for the second?
\def\*#1{\mathbf{#1}}
\newcommand*{\V}[1]{\mathbf{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}


\centerline{\it CS 577}
\centerline{\it HW \#10 Submission}
\centerline{\it Name: Joses Omojola}

Questions in Part A and B were completed in python and saved in different python files. The figures and the model scripts were complex so I split them into separate files 
and submitted a total of 4 python scripts namely \emph{hw10a.py, hw10b.py, plots.py, model.py}. The list of requirements to run the script is also included although it has 
some packages that I use in other classes. Four models were trained in total using gpus from colab and kaggle. Limited gpu access restricted the number of tests I could 
complete but I tried to cover all the deliverables. Results are saved in three folders namely \emph{metrics, output, and saved}.

\begin{enumerate}
    \item[Part A.]
    \subsection*{Dataset.}
    I used the recommended CIFAR10 dataset with 50k training images and 10k test images. Simple transformations like random cropping and normalization were applied to the dataloader 
    pipeline to improve the training workflow. The training data was split into 10 folds with 90\% of the it being used for training the model, and 10\% for model validation. In 
    each fold, the training dataset has 45k images, 5k validation images, and 10k test images.
    \subsection*{Model Architectures.}
    I used 3 model architectures denoted as \emph{SimpleCNN},\emph{OverfitCNN}, and \emph{ResNet18} in the \emph{"hw10.py"} program.  \\
    1. SimpleCNN has a single convolution layer, one max-pooling layer, and 1 fully connected layer (fcn). The output layer predicts 10 class probabilities to match the 
    CIFAR10 dataset classes.  \\
    2. OverfitCNN uses 5 convolution layers with 3 fcn's. Each conlution layer contains embedded batchnorm and max pooling layers with ReLU activation. Dropout is not 
    used in the last 3 fcn's to force the model to overfit. \\
    3. The best fit model uses the ResNet18 architecture \cite{he2015deepresiduallearningimage}. It has one input convolution layer, four additional ResNet blocks, and 1 fcn. 
    Each ResNet block contains convolution and identity blocks with skips connections to improve performance. The model weights were trained from scratch, and average pooling 
    was applied to the final fcn layer before predicting class probabilities. \\
    The ResNet implementation was modified from \url{https://github.com/sun1lach/cifar10_grad_cam}. Other architectures were modified after official pytorch documentation 
    \url{https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html}
    \subsection*{Experiments.}
    All the models were trained using randomly sampled data for each fold, and a seed of 577 was used for reproducibility. Each model was trained for 12 epochs, and the 
    validation and test metrics were calculated at the end of each epoch. Training weights were reset for each fold to avoid weight leakage across the different subsamples, and 
    a learning rate scheduler with a max learning rate of 0.01 was used. Categorical cross-entropy loss was also used for all models. The training loss for the 3 model architectures 
    is shown in \autoref{fig1}.

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.75]{output/training_loss_k1.png}
        \caption{Training loss for 3 models for fold 1. Note: The overfit and best fit models have similar training errors at the end of the 12 epochs. The lack of dropout in the 
        overfit model dampens its performance on unseen test data.}
        \label{fig1}
    \end{figure}

    The validation and test loss trends are shown in \autoref{fig2}. The best-fit and overfit panels are scaled equally to demonstrate the overfitting effect which causes a higher 
    loss in the overfit test dataset. The underfit data has a much higher loss than the other two, hence it uses a separate scale. The average accuracy and standard error for all 
    folds is shown in (\autoref{tab1})

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.6]{output/val_test_loss_k1.png}
        \caption{Validation and test loss for 3 models for fold 1. The overfit model has a higher training and validation loss compared to the best fit model.}
        \label{fig2}
    \end{figure}

    \begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    \  & Underfit & Best fit & Overfit \\ \hline
    Train Accuracy & 40.52 $\pm$ 0.28 & 84.80 $\pm$ 0.05 & 79.61 $\pm$ 0.04 \\ \hline
    Validation Accuracy & 41.07 $\pm$ 0.43 & 75.82 $\pm$ 0.17 & 69.64 $\pm$ 0.16 \\ \hline
    Test Accuracy & 41.12 $\pm$ 0.39 & 75.44 $\pm$ 0.13 & 69.61 $\pm$ 0.16 \\ \hline
    \end{tabular}
    \caption{Average accuracy from each model across all the folds. The main number is the average accuracy, and the standard error is the smaller
    decimal after $\pm$, for each entry.}
    \label{tab1}
    \end{table}

    The training accuracy trend for the overfit model (@ fold k=1) begins to diverge between epochs 3-8 \autoref{fig3}, however, it begins improving for subsequent epochs because 
    the dataset is relatively simple for the model architecture. Removing convolution layers can help reduce the model performance, but the overfit model is required to have a high 
    training data performance.

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.6]{output/training_acc_k1.png}
        \caption{Overfit model accuracy trend for fold 1. The divergence of the training and test accuracies, indicative of overfitting, is shown with the dashed black line.}
        \label{fig3}
    \end{figure}

    \item[Part B.]
    \subsection*{Learned Features.}
    The features of the best fit model can be visualized using the Gradient-weighted Class Activation Mapping (Grad-CAM) \cite{Selvaraju_2019}. The technique uses gradients of 
    target CNN layers to highlight important regions in an image that help discriminate between classes. The grad-CAM features for the ResNet18 are shown in \autoref{fig4}, where 
    edges around the objects shaded by gradient help the network distinguish between classes. Bright gradients can be observed on the horse and deer legs/face which indicates that 
    these are useful features for identifying objects.

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.7]{output/grad_cam_features.png}
        \caption{Learned features from convolution layer. Distinct edges in features show as hotspots on the gradient map. Brighter (red-yellow) colors have stronger effects.}
        \label{fig4}
    \end{figure}

    I implemented a convolutional autoencoder for image reconstruction. The encoder had 4 layers with a maximum number of 64 channels in the largest layer and 8 channels at the 
    bottleneck layer. The number of encoder output channels decreased by 2 for each layer $[64 \rightarrow 32 \rightarrow 16 \rightarrow 8]$. The decoder layer reconstructed the 
    features to obtain the input image size. The autoencoder was trained for 100 epochs on the CIFAR10 dataset, with a batch size of 64 and a mean square error loss. The result on 
    a test batch of images is shown in \autoref{fig5}.

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.7]{output/autoencoder_images.png}
        \caption{Autoencoder reconstruction results. (Top) Input images from CIFAR10 test dataset. (Bottom) Reconstructed images from the autoencoder output layer. All images are 
        (32x32x3) and units are in pixels.}
        \label{fig5}
    \end{figure}

    The network does not compress the input images size in the bottleneck layer, however, it uses less convolutional filters to extract important features. Hence the layers near 
    the bottleneck are blurry but contain the most important features for identifying an object. This enables the network to reconstruct the images reasonably well. Although the 
    reconstructed images are not a the same resolution as the input images, they contain major features that can be used to distinguish adjacent images like the plane and ship in 
    \autoref{fig5}. Learned features in the convolution layers can be extracted and visualized as feature maps. I extracted the features using the same concept as this medium 
    article \url{https://ravivaishnav20.medium.com/visualizing-feature-maps-using-pytorch-12a48cd1e573}, and plotted the feature maps in \autoref{fig6}. In this image, we can 
    observe how the network processes an input horse image and retains the most important features within the deepest bottleneck convolution layer. 

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.7]{output/feature_maps.png}
        \caption{Feature maps for an input horse image from the shallowest to the deepest convolution layer. Important features like the horse's neck and legs are visible in 
        the bottleneck layer (Conv2d\_9), and less important features are stripped off.}
        \label{fig6}
    \end{figure}

\end{enumerate}

\printbibliography % Prints the bibliography

\end{document}