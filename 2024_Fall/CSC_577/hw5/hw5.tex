%
% Latex comments start with the percent symbol.
%
% This file should create a pdf on a mac or Linux command line by running:
%     pdflatex hw1.tex
% I usually add a few options
%     pdflatex -halt-on-error -interaction=nonstopmode -file-line-error hw1.tex
% 
% If you are new to Latex, you might not know that you may need to run the above
% twice for the compiler to sort out its references. (There are ways to finesse
% this). 
%

\documentclass[12pt]{report}

% Whether or not you need all these packages, or even some more will vary. These
% are some common ones, but not all are needed for this document. There is no
% real harm loading your favorites out of habit. 


\usepackage{algorithm,algorithmic,alltt,amsmath,amssymb,bm,
    cancel,color,fullpage,graphicx,listings,mathrsfs,
    multirow,setspace,subcaption,upgreek,xcolor}
\usepackage[numbered,framed]{matlab-prettifier}
\usepackage[colorlinks]{hyperref}
\usepackage[nameinlink,noabbrev]{cleveref}
\usepackage[verbose]{placeins}
\usepackage{caption}
\usepackage[skip=0.1ex, belowskip=1ex,
            labelformat=brace,
            singlelinecheck=off]{subcaption}

\doublespacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%% Operators %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Your personal shortcuts. You do not need to use any. argmax and argmin
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%% Distributions
\newcommand{\N}{\mathcal{N}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\Poi}{{\text Poisson}}
\newcommand{\Exp}{{\text Exp}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\Ber}{{\text Bern}}
\newcommand{\Lap}{{\text Laplace}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
% \usepackage[left=1cm,right=2.5cm,top=2cm,bottom=1.5cm]{geometry}
% Code blocks formatting

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0}

% For faster processing, load Matlab syntax for listings
\lstloadlanguages{Matlab}%
\lstset{language=Matlab,        % Use MATLAB
        % frame=single,   % Single frame around code
        basicstyle=\small\ttfamily,     % Use small true type font
        keywordstyle=[1]\color{blue}\bfseries,  % MATLAB functions bold and blue
        keywordstyle=[2]\color{purple}, % MATLAB function arguments purple
        keywordstyle=[3]\color{blue}\underbar,  % User functions underlined and blue
        identifierstyle=,       % Nothing special about identifiers
        % Comments small dark green courier
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small,
        stringstyle=\color{purple},     % Strings are purple
        showstringspaces=false,         % Don't put marks in string spaces
        tabsize=2,      % 2 spaces per tab
        %%% Put standard MATLAB functions not included in the default language here
        morekeywords={xlim,ylim,var,alpha,factorial,poissrnd,normpdf,normcdf},
        %%% Put MATLAB function parameters here
        morekeywords=[2]{on, off, interp},
        %%% Put user defined functions here
        morekeywords=[3]{hw1,hw2,},
        gobble=4,
        morecomment=[l][\color{blue}]{...},     % Line continuation (...) like blue comment
        numbers=left,   % Line numbers on left
        firstnumber=1,          % Line numbers start with line 1
        numberstyle=\tiny\color{blue},  % Line numbers are blue
        stepnumber=5    % Line numbers go in steps of 5
}

%% Probability
\newcommand{\E}[1]{\mathbb{E}[#1]}
\newcommand{\Cov}[2]{\mathbb{C}\mathrm{ov}(#1,#2)}

%% Bold font for vectors from Ernesto, but I do not know how the first one
%  works, but it seems necessary for the second?
\def\*#1{\mathbf{#1}}
\newcommand*{\V}[1]{\mathbf{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}


\centerline{\it CS 577}
\centerline{\it HW \#5 Submission}
\centerline{\it Name: Joses Omojola}

Questions from Part A-D were completed in matlab and saved in the \emph{hw5.m} program. The program creates an \emph{output} folder to save images, 
so that the root directory is not always cluttered. The program can be run using \textit{hw5()}, and the results to coding questions are printed in 
terminal. File paths to input images are hardcoded, replace "IMG\_0862.png" with "IMG\_0861.jpeg" on Line 70 of \emph{hw5.m} if you get errors.

\begin{enumerate}

    \item[Part-A.]
    \ \\
    The world and image coordinates from \emph{hw4} were read into matlab and inverted using the homogenous least squares method to get the camera 
    matrix \textbf{M}. The inverted camera matrix 
    \[
    M = 
    \begin{bmatrix}
    0.0313 & 0.0532 & -0.1365 & 0.4854 \\
    -0.1393 & 0.0490 & -0.0236 & 0.8483 \\
    -0.0000 & -0.0000 & -0.0000 & 0.0009
    \end{bmatrix}
    \]

    The vizualization comparing the projected and original image coordinates is shown in \autoref{fig:Figure1}.\\
    The resulting \textbf{RMS error} between the projected points from "M" versus the original image coordinates was \textbf{9.5768}. This is lower than 
    the 17.01 and 41.28 RMS values that were obtained with camera matrix 1 and 2 in \emph{hw4}.  
    
    The calibration process does \textbf{not} minimize the sum of the squared errors(i.e., the sum of squared differences between observed and projected 
    points in the image plane). Homogeneous least squares minimizes an \textbf{algebraic error} in a linear system, not the reprojection error in image pixels. 
    $$P \cdot m = 0$$
    where $m$ represents the flattened camera matrix $P$. The solution to this minimizes the error in a linear sense (in terms of matrix $P$), which 
    corresponds to the smallest singular value of the matrix in the SVD decomposition. The projection from 3D to 2D is nonlinear, and the algebraic error 
    doesn't account for this transformation. While homogeneous least squares is computationally efficient and useful for getting an initial estimate of 
    the camera matrix, it is less than ideal for achieving accurate 2D projections. An ideal approach refines the initial matrix by minimizing reprojection 
    error through nonlinear optimization.

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.25]{output/f1_predicted_points_HLSQR.png}
        \caption{Visualization of projected points from "M" matrix.}
        \label{fig:Figure1}
    \end{figure}

    \FloatBarrier 

    \item[Part-B.]
    \ \\
    \begin{enumerate}
        \item[1.] Parametric equations for a sphere were used to generate 3D points at (x,y,z). The number of points were manually modified from 10 to 
        100, till there were no holes on the sphere.
        \item[2.] The camera position was assumed to be $[9, 14, 11]$.
        \item[3.] The dot products between the camera position and the outward normal vector of each point on the sphere was used to create a visibility 
        mask. Points less than zero are not visible, and were masked out.
        \item[4.] The Lambertian reflectance was calculated from the dot product between the sphere's surface normal and the normalized light direction. 
        Negative values were set to zero (self-shadow).
        \item[5.] The visible points were projected onto the 2D calibration image, using the camera matrix from \emph{Part A}. The points are shaded by 
        the lambertian reflectance values.
    \end{enumerate}
    The resulting plot of the projected sphere shaded by lambertian reflectance is shown in \autoref{fig:Figure2}. When the light source is in front of 
    the sphere, the reflectance is brightest in the increasing x-direction \autoref{fig:Figure2a}. However, when the light source is rotated behind the 
    sphere $[-30, 0, 0]$, the reflectance is highest in the decreasing x-direction facing the source \autoref{fig:Figure2b}. Considering that the reflectance 
    tracks with the light source, the resulting image makes sense.

    \begin{figure}[!ht]\centering
        \hspace*{-1.2in}
        \begin{subfigure}{0.6\textwidth}
            \includegraphics[scale=0.23]{output/f2_projected_sphere_with_visible_points.png}
            \caption{Projected sphere with original light source at $[33, 29, 44]$.}
            \label{fig:Figure2a}
        \end{subfigure}
    \vfil
        \hspace*{-1.2in}
        \begin{subfigure}{0.6\textwidth}
        \includegraphics[scale=0.23]{output/f3_projected_sphere_with_rotated_light.png}
        \caption{Projected sphere with rotated light source at $[-30, 0, 0]$}
        \label{fig:Figure2b}
        \end{subfigure}
        \caption{Projected sphere at world coordinates $[3,2,3]$ using camera matrix from \emph{Part A}, 
        showing the effect of varying light sources. World coordinates are in inches.}
        \label{fig:Figure2}
    \end{figure}

    \FloatBarrier 

    \item[Part-C.]
    \ \\
    \begin{enumerate}
    \item[1.]
    \ \\
    The camera matrix \( M \) can be decomposed into three matrices, and is generally expressed as:
    $$
    M = K \cdot [R | t]
    $$
    Where:\\
    - \( K \) is the \textbf{intrinsic parameter matrix}.\\
    - \( R \) is the \textbf{rotation matrix} that describes the orientation of the camera.\\
    - \( t \) is the \textbf{translation vector} that describes the position of the camera.

    We assume that the camera axes are perpendicular (skew angle = 90 degrees), so we can ignore skew, reducing the number of unknowns from 11 to 10. The matrix 
    \( K \) is a \( 3 \times 3 \) upper triangular matrix that describes how the camera's internal parameters map 3D points into 2D image coordinates. \( K \) can 
    be written as

    $$
    K = \begin{bmatrix}
    \alpha & \gamma & u_0 \\
    0      & \beta  & v_0 \\
    0      & 0      & 1
    \end{bmatrix} = 
    \begin{bmatrix}
    \alpha & 0      & u_0 \\
    0      & \beta  & v_0 \\
    0      & 0      & 1
    \end{bmatrix}
    $$

    where:\\
    - \( \alpha \): The \textbf{horizontal scaling factor} (focal length in terms of pixels along the x-axis).\\
    - \( \beta \): The \textbf{vertical scaling factor} (focal length in terms of pixels along the y-axis).\\
    - \( \gamma \): The \textbf{skew} between the x and y axes. Since we assume that the axes are perpendicular, we set \( \gamma = 0 \).\\
    - \( u_0, v_0 \): The coordinates of the \textbf{principal point} which is the intersection of the camera's optical axis with the image plane. These values correspond 
    to the pixel coordinates where the optical center of the camera is located.

    
    \item[2.]
    \ \\
    From the general camera matrix expression the extrinsic matrix $X$ can be written as $X = [R \ \ \  t]$ \\
    Where:\\
    - \( R \) is the  3 x 3 \textbf{rotation matrix} (orientation of the camera in 3D space).\\
    - \( t \) is the  3 x 1 \textbf{translation vector} (camera's position in the world coordinate system).\\

    The rotation matrix \( R \) is composed of three orthonormal rotation vectors \( \mathbf{r_1} \), \( \mathbf{r_2} \), and \( \mathbf{r_3} \), that describes the camera's 
    orientation along one of its axes (usually corresponding to the x, y, and z axes in the camera coordinate system). It has a general form of 
    $$
    R = \begin{bmatrix}
    \mathbf{r_1} & \mathbf{r_2} & \mathbf{r_3}
    \end{bmatrix}
    =
    \begin{bmatrix}
    r_{11} & r_{12} & r_{13} \\
    r_{21} & r_{22} & r_{23} \\
    r_{31} & r_{32} & r_{33}
    \end{bmatrix}
    $$
    where each vector is orthonormal, and can be defined as : \\
    - \( \mathbf{r_1} = (r_{11}, r_{21}, r_{31})^\top \) is the first orthonormal vector (x-axis rotation). \\
    - \( \mathbf{r_2} = (r_{12}, r_{22}, r_{32})^\top \) is the second orthonormal vector (y-axis rotation). \\
    - \( \mathbf{r_3} = (r_{13}, r_{23}, r_{33})^\top \) is the third orthonormal vector (z-axis rotation). \\

    If the camera is located at \( (t_x, t_y, t_z) \), the translation vector is:
    $$
    t = \begin{bmatrix}
    t_x \\
    t_y \\
    t_z
    \end{bmatrix}
    $$

    We can combine the rotation matrix \( R \) and translation vector \( t \) to form the extrinsic matrix \( X \) that is defined by three orthonormal rotation vectors and a 
    translation vector as:

    $$
    X = \begin{bmatrix}
    R & t
    \end{bmatrix}
    =
    \begin{bmatrix}
    r_{11} & r_{12} & r_{13} & t_x \\
    r_{21} & r_{22} & r_{23} & t_y \\
    r_{31} & r_{32} & r_{33} & t_z
    \end{bmatrix}
    $$
    
    \item[3.]
    \ \\
    We can multiply $K$ and $X$ from Part C1 and C2 to get 
    $$
    M = \begin{bmatrix}
    \alpha & 0 & u_0 \\
    0 & \beta & v_0 \\
    0 & 0 & 1
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
    r_{11} & r_{12} & r_{13} & t_x \\
    r_{21} & r_{22} & r_{23} & t_y \\
    r_{31} & r_{32} & r_{33} & t_z
    \end{bmatrix}
    $$

    The multiplication of $K$ rows with $X$ columns gives 
    $$
    M_1 = 
    \begin{bmatrix}
    \alpha & 0 & u_0
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
    r_{11} & r_{12} & r_{13} & t_x \\
    r_{21} & r_{22} & r_{23} & t_y \\
    r_{31} & r_{32} & r_{33} & t_z
    \end{bmatrix}
    =
    \begin{bmatrix}
    \alpha r_{11} + u_0 r_{31} & \alpha r_{12} + u_0 r_{32} & \alpha r_{13} + u_0 r_{33} & \alpha t_x + u_0 t_z
    \end{bmatrix}
    $$
    $$
    M_2 = 
    \begin{bmatrix}
    0 & \beta & v_0
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
    r_{11} & r_{12} & r_{13} & t_x \\
    r_{21} & r_{22} & r_{23} & t_y \\
    r_{31} & r_{32} & r_{33} & t_z
    \end{bmatrix}
    =
    \begin{bmatrix}
    \beta r_{21} + v_0 r_{31} & \beta r_{22} + v_0 r_{32} & \beta r_{23} + v_0 r_{33} & \beta t_y + v_0 t_z
    \end{bmatrix}
    $$
    $$
    M_3 = 
    \begin{bmatrix}
    0 & 0 & 1
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
    r_{11} & r_{12} & r_{13} & t_x \\
    r_{21} & r_{22} & r_{23} & t_y \\
    r_{31} & r_{32} & r_{33} & t_z
    \end{bmatrix}
    =
    \begin{bmatrix}
    r_{31} & r_{32} & r_{33} & t_z
    \end{bmatrix}
    $$
    where $(M_1,M_2,M_3)$ are row vectors, and the final algebraic form of $M$ is 
    $$
    M = \begin{bmatrix}
    \alpha r_{11} + u_0 r_{31} & \alpha r_{12} + u_0 r_{32} & \alpha r_{13} + u_0 r_{33} & \alpha t_x + u_0 t_z \\
    \beta r_{21} + v_0 r_{31} & \beta r_{22} + v_0 r_{32} & \beta r_{23} + v_0 r_{33} & \beta t_y + v_0 t_z \\
    r_{31} & r_{32} & r_{33} & t_z
    \end{bmatrix}
    $$


    % ### Camera Matrix \( M \) from \( K \) and \( X \)

    % To find the **camera matrix** \( M \), we need to multiply the **intrinsic matrix** \( K \) with the **extrinsic matrix** \( X = [R|t] \). The resulting matrix \( M \) will transform 3D world coordinates into 2D image coordinates. Let's go step-by-step.

    % #### Step 1: Review of the Intrinsic Matrix \( K \)

    % From earlier, the intrinsic matrix \( K \) is:

    % \[
    % K = \begin{bmatrix}
    % \alpha & 0 & u_0 \\
    % 0 & \beta & v_0 \\
    % 0 & 0 & 1
    % \end{bmatrix}
    % \]

    % Where:
    % - \( \alpha \) is the horizontal focal length scaled in pixels.
    % - \( \beta \) is the vertical focal length scaled in pixels.
    % - \( (u_0, v_0) \) are the coordinates of the principal point.

    % #### Step 2: Review of the Extrinsic Matrix \( X = [R|t] \)

    % The extrinsic matrix \( X \) is the product of the rotation matrix \( R \) and the translation vector \( t \), giving:

    % \[
    % X = \begin{bmatrix}
    % r_{11} & r_{12} & r_{13} & t_x \\
    % r_{21} & r_{22} & r_{23} & t_y \\
    % r_{31} & r_{32} & r_{33} & t_z
    % \end{bmatrix}
    % \]

    % Where:
    % - \( R = \begin{bmatrix} r_{11} & r_{12} & r_{13} \\ r_{21} & r_{22} & r_{23} \\ r_{31} & r_{32} & r_{33} \end{bmatrix} \) is the rotation matrix.
    % - \( t = \begin{bmatrix} t_x \\ t_y \\ t_z \end{bmatrix} \) is the translation vector.

    % #### Step 3: Multiplication of \( K \) and \( X \)

    % The camera matrix \( M \) is the product of \( K \) and \( X \):

    % \[
    % M = K \cdot [R|t]
    % \]

    % This is a \( 3 \times 3 \) matrix \( K \) multiplying a \( 3 \times 4 \) matrix \( [R|t] \), resulting in a \( 3 \times 4 \) matrix. Let's perform the matrix multiplication step by step.


    % ### Step 4: Perform the Multiplication

    % We perform the multiplication row by row for \( K \) and column by column for \( [R|t] \).

    % #### First row of \( M \):
    % \[
    % \begin{bmatrix}
    % \alpha & 0 & u_0
    % \end{bmatrix}
    % \cdot
    % \begin{bmatrix}
    % r_{11} & r_{12} & r_{13} & t_x \\
    % r_{21} & r_{22} & r_{23} & t_y \\
    % r_{31} & r_{32} & r_{33} & t_z
    % \end{bmatrix}
    % =
    % \begin{bmatrix}
    % \alpha r_{11} + u_0 r_{31} & \alpha r_{12} + u_0 r_{32} & \alpha r_{13} + u_0 r_{33} & \alpha t_x + u_0 t_z
    % \end{bmatrix}
    % \]

    % #### Second row of \( M \):
    % \[
    % \begin{bmatrix}
    % 0 & \beta & v_0
    % \end{bmatrix}
    % \cdot
    % \begin{bmatrix}
    % r_{11} & r_{12} & r_{13} & t_x \\
    % r_{21} & r_{22} & r_{23} & t_y \\
    % r_{31} & r_{32} & r_{33} & t_z
    % \end{bmatrix}
    % =
    % \begin{bmatrix}
    % \beta r_{21} + v_0 r_{31} & \beta r_{22} + v_0 r_{32} & \beta r_{23} + v_0 r_{33} & \beta t_y + v_0 t_z
    % \end{bmatrix}
    % \]

    % #### Third row of \( M \):
    % \[
    % \begin{bmatrix}
    % 0 & 0 & 1
    % \end{bmatrix}
    % \cdot
    % \begin{bmatrix}
    % r_{11} & r_{12} & r_{13} & t_x \\
    % r_{21} & r_{22} & r_{23} & t_y \\
    % r_{31} & r_{32} & r_{33} & t_z
    % \end{bmatrix}
    % =
    % \begin{bmatrix}
    % r_{31} & r_{32} & r_{33} & t_z
    % \end{bmatrix}
    % \]

    % ### Step 5: Putting it All Together

    % Now, combining the results of all rows, we obtain the full camera matrix \( M \):

    % \[
    % M = \begin{bmatrix}
    % \alpha r_{11} + u_0 r_{31} & \alpha r_{12} + u_0 r_{32} & \alpha r_{13} + u_0 r_{33} & \alpha t_x + u_0 t_z \\
    % \beta r_{21} + v_0 r_{31} & \beta r_{22} + v_0 r_{32} & \beta r_{23} + v_0 r_{33} & \beta t_y + v_0 t_z \\
    % r_{31} & r_{32} & r_{33} & t_z
    % \end{bmatrix}
    % \]

    % ### Step 6: Interpretation of \( M \)

    % This matrix \( M \) is the full **camera projection matrix**. It incorporates both the **intrinsic parameters** \( K \) and the **extrinsic parameters** \( [R|t] \). This matrix transforms 3D world coordinates \( (X_w, Y_w, Z_w) \) into 2D image coordinates \( (x, y) \) through the relationship:

    % \[
    % \begin{bmatrix}
    % x \\
    % y \\
    % w
    % \end{bmatrix}
    % =
    % M \cdot
    % \begin{bmatrix}
    % X_w \\
    % Y_w \\
    % Z_w \\
    % 1
    % \end{bmatrix}
    % \]
    % Where:
    % - \( x = X / w \)
    % - \( y = Y / w \)
    % - \( w \) is the homogeneous coordinate.

    % ### Conclusion

    % In summary, we derived the camera projection matrix \( M \) by multiplying the intrinsic matrix \( K \) with the extrinsic matrix \( X \). The final matrix \( M \) projects 3D world points into 2D image coordinates, combining both the camera's internal parameters (focal length, principal point) and external parameters (rotation, translation).
    \item[4.]
    \ \\

    \end{enumerate}

\end{enumerate}

\end{document}