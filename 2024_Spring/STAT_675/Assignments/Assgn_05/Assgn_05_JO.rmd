---
title: "Assignment_05_JO"
output:
  pdf_document: default
---

```{r}
library(ggplot2)
library(gridExtra)
library(MASS)
library(mvtnorm)
library(patchwork)
library(plot3D)
library(StereoMorph)
set.seed(675) # For reproducibility
```

1. A chord of a circle is a straight line segment whose endpoints both lie on the circle. Conduct a simulation study on 
the distribution of the length of a random chord on unit circle.
    (a) "random endpoints" method: Choose two random points on the circumference of the unit circle and draw the chord joining 
    them. (In fact, you may fix one endpoint and assume the other uniformly distributed on the circle). Write a function with 
    input n and output the lengths of n random chords.
    (b) "random midpoint" method: Choose a random point (uniformly distributed) within the circle and construct a chord with the 
    chosen point as its midpoint. Write a function with input n and output the lengths of n random chords.
    (c) Conduct a simulation study to estimate the probability that the length of a random chord is greater than $\sqrt{3}$. Do 
    you get the same probability using both chord generation methods? (You may search the keywords Bertrand paradox for more 
    information.) - No, I didn't.

```{r}
# Simulation circle dimensions 
xx0=0; # center of disk 
yy0=0; 
r=1;   # disk radius
# Create points for circle
t <- seq(0, 2*pi, len = 200);
xp <- r*cos(t);
yp <- r*sin(t);

# Function to generate random chords using Random Endpoint (Random Endpoints)
generate_chords_rnd_ept <- function(n) {
  thetaA1 <- 2 * pi * runif(n)  # choose angular component uniformly
  thetaA2 <- 2 * pi * runif(n)  # choose angular component uniformly
  
  # calculate segment endpoints
  xxA1 <- xx0 + r * cos(thetaA1)
  yyA1 <- yy0 + r * sin(thetaA1)
  xxA2 <- xx0 + r * cos(thetaA2)
  yyA2 <- yy0 + r * sin(thetaA2)
  
  # calculate midpoints of segments
  xxA0 <- (xxA1 + xxA2) / 2
  yyA0 <- (yyA1 + yyA2) / 2
  
  # Convert to data frame
  chord_data_ept <- data.frame(x1 = xxA1, y1 = yyA1, x2 = xxA2, y2 = yyA2)
  midpoint_data <- data.frame(x = xxA0, y = yyA0)
  
  # Calculate lengths of chords
  chord_lengths <- sqrt((xxA1 - xxA2)^2 + (yyA1 - yyA2)^2)
  
  # Return chord data, chord lengths, and midpoints
  return(list(chords = chord_data_ept,
              chord_lengths = chord_lengths,
              midpoints = midpoint_data))
}

# Function to generate random chords using Random Midpoint (Random Midpoint)
generate_chords_rnd_mpt <- function(n) {
  thetaC <- 2 * pi * runif(n)  # choose angular component uniformly
  pC <- r * sqrt(runif(n))     # choose radial component
  qC <- sqrt(r^2 - pC^2)       # distance to circle edge (along line)
  
  # calculate trig values
  sin_thetaC <- sin(thetaC)
  cos_thetaC <- cos(thetaC)
  
  # calculate segment endpoints
  xxC1 <- xx0 + pC * cos_thetaC + qC * sin_thetaC
  yyC1 <- yy0 + pC * sin_thetaC - qC * cos_thetaC
  xxC2 <- xx0 + pC * cos_thetaC - qC * sin_thetaC
  yyC2 <- yy0 + pC * sin_thetaC + qC * cos_thetaC
  
  # calculate midpoints of segments
  xxC0 <- (xxC1 + xxC2) / 2
  yyC0 <- (yyC1 + yyC2) / 2
  
  # Convert to data frame
  chord_data_mpt <- data.frame(x1 = xxC1, y1 = yyC1, x2 = xxC2, y2 = yyC2)
  midpoint_data <- data.frame(x = xxC0, y = yyC0)
  
  # Calculate lengths of chords
  chord_lengths <- sqrt((xxC1 - xxC2)^2 + (yyC1 - yyC2)^2)
  
  # Return chord data, chord lengths, and midpoints
  return(list(chords = chord_data_mpt,
              chord_lengths = chord_lengths,
              midpoints = midpoint_data))
}

# Plotting using ggplot
plot_chords_midpoints <- function(chord_data, midpoint_data, title_chords, 
                                  title_midpoints, chord_color, mpt_col) {
  # Background Circle
  plot_circle <- ggplot(data = data.frame(x = c(xx0 + xp), 
                        y = c(yy0 + yp)), aes(x, y)) +
    geom_path(color = 'black') +
    theme_void() +
    labs(title = title_chords) +
    theme(plot.title = element_text(hjust = 0.5)) +
    coord_equal()
  # Chord Lines
  plot_chords <- plot_circle +
    geom_segment(data = chord_data, aes(x = x1, y = y1, xend = x2, yend = y2), 
                 color = chord_color, linewidth = 0.05) +
    coord_equal()
  # Chord Midpoints
  plot_midpoints <- plot_circle +
    geom_point(data = midpoint_data, aes(x, y), color = mpt_col, shape = 20, size = 0.2) +
    coord_equal()
  
  return(list(plot_chords, plot_midpoints))
}

# Small Simulation for plotting
n <- 500  # Number of simulations

# Generate random chords using Random Endpoint
chords_rnd_ept <- generate_chords_rnd_ept(n)
chord_data_rnd_ept <- chords_rnd_ept$chords
midpoint_data_rnd_ept <- chords_rnd_ept$midpoints

# Generate random chords using Random Midpoint
chords_rnd_mpt <- generate_chords_rnd_mpt(n)
chord_data_rnd_mpt <- chords_rnd_mpt$chords
midpoint_data_rnd_mpt <- chords_rnd_mpt$midpoints

# Plot for both Solutions
plots_a <- plot_chords_midpoints(chord_data_rnd_ept, midpoint_data_rnd_ept, 
                                  'Chords Rnd. Endpt.', 'Midpoints Rnd. Endpt.', 
                                  'red', 'red')
plots_c <- plot_chords_midpoints(chord_data_rnd_mpt, midpoint_data_rnd_mpt, 
                                  'Chords Rnd. Midpt.', 'Midpoints Rnd. Midpt.', 
                                  'green', 'green')

# Arrange the plots
plot_layout <- (plots_a[[1]] | plots_a[[2]]) / (plots_c[[1]] | plots_c[[2]])
# Show the plots
plot_layout



# ------------------------- Large Simulation for probabilites -------------------------
n <- 5000  # Number of simulations

# Generate random chords using Random Endpoint
chords_rnd_ept <- generate_chords_rnd_ept(n)

# Generate random chords using Random Midpoint
chords_rnd_mpt <- generate_chords_rnd_mpt(n)

# Estimate probability that the length of a random chord is greater than sqrt(3)
prob_rnd_ept <- sum(chords_rnd_ept$chord_lengths > sqrt(3)) / n
prob_rnd_mpt <- sum(chords_rnd_mpt$chord_lengths > sqrt(3)) / n

# Output probabilities
cat("Probability (Random Endpoint):", prob_rnd_ept, "\n")
cat("Probability (Random Midpoint):", prob_rnd_mpt, "\n")

# Plot density plots for both methods
# Combine data for both methods
combined_data <- data.frame(
  x = c(chords_rnd_ept$chord_lengths, chords_rnd_mpt$chord_lengths),
  method = rep(c("Rnd. Endpts", "Rnd. Midpts"), each = n)
)
# Plot density curves for both methods on the same plot
p1 <- ggplot(combined_data, aes(x = x, fill = method)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 2)) +
  labs(title = "Density Plot of Chord Lengths", x = "Length") +
  theme_bw() +
  theme(legend.position = "bottom",legend.title=element_blank()) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_size_manual(values = c("Rnd. Endpts" = 2, "Rnd. Midpts" = 2))

# Calculate and plot proportions exceeding sqrt(3)
# Combine proportions into a data frame
prop_data <- data.frame(
  method = c("Random Endpoints", "Random Midpoint"),
  prop = c(prob_rnd_ept, prob_rnd_mpt)
)
# Create the bar plot using the data frame
p2 <- ggplot(prop_data, aes(x = method, y = prop)) +
  geom_bar(stat = "identity", fill = c("blue", "orange")) +
  labs(title = "Proportion of Chords > sqrt(3)", x = "", y = "Proportion") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# grid.arrange(p1, p2, nrow = 1)
hist_layout <- (p1 | p2)
hist_layout
```

2. Random vectors generation from multivariate Gaussian distribution I: general case.
    (a) Implement several random vector generators which return n IID random vectors from Gaussian distribution with mean 
    $\mu$ and covariance $\Sigma$. Try the three approaches: eigenvalue decomposition. singular value decomposition, and 
    Choleski factorization.
    (b) Search for R packages and functions which can generate random vectors from Gaussian distribu-tion.
    (c) Compare these generators in terms of computation time (and quality if you could).

```{r}
mvnormal_eigendec <- function(mu, Sigma, n) {
  # Extract dimensions
  d <- nrow(Sigma)

  # Eigenvalue/eigenvector decomposition
  eigen <- eigen(Sigma)
  eigenvalues <- eigen$values
  eigenvectors <- eigen$vectors

  # Square root of eigenvalues
  sqrt_eigenvalues <- sqrt(eigenvalues)

  # Generate standard normal samples
  Z <- matrix(rnorm(n * d), nrow = n, ncol = d)

  # Return transformed samples
  return(mu + eigenvectors %*% diag(sqrt_eigenvalues) %*% t(Z))
}

mvnormal_svd <- function(mu, Sigma, n) {
  # Extract dimensions
  d <- nrow(Sigma)

  # Singular value decomposition
  svd <- svd(Sigma)
  U <- svd$u
  S <- svd$d
  Vt <- svd$v

  # Square root of singular values
  sqrt_S <- sqrt(S)

  # Generate standard normal samples
  Z <- matrix(rnorm(n * d), nrow = n, ncol = d)

  # Return transformed samples
  return(mu + U %*% diag(sqrt_S) %*% t(Z))
}

mvnormal_choleski <- function(mu, Sigma, n) {
  # Extract dimensions from the covariance matrix
  d <- nrow(Sigma)

  # Choleski factor of positive definite covariance matrix
  L <- chol(Sigma)

  # Generate standard normal samples
  Z <- matrix(rnorm(n * d), nrow = n, ncol = d)

  # Return transformed samples
  return(mu + L %*% t(Z))
}

# Define mean and covariance matrix
mu <- c(2, 3)
Sigma <- matrix(c(4, 2, 2, 5), nrow = 2, ncol = 2)

# Sample size
n <- 1000

# Generate samples using mvtnorm and manual functions
samples_mvtnorm <- rmvnorm(n, mu, Sigma)
samples_eigendec <- t(mvnormal_eigendec(mu, Sigma, n))
samples_svd <- t(mvnormal_svd(mu, Sigma, n))
samples_choleski <- t(mvnormal_choleski(mu, Sigma, n))

# Define plotting function to include density plots in a grid
plot_distribution_grid <- function(data_list, titles) {
  # Create a data frame with a single column for the density plot
  colMeans(data_list)
  var(data_list)
  dst <- data.frame(data_list)
  str(dst)

  ggplot(dst , aes(x = X1, y=X2)) +
    geom_point(alpha = .2, size=0.2) +
    geom_density_2d() +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title = titles)
}

# Define titles for the plots
titles <- c("RVT Norm", "Eigen Decomposition", "SVD", "Choleski")

# Plot the distributions
p1 <- plot_distribution_grid(samples_mvtnorm, titles[1])
p2 <- plot_distribution_grid(samples_eigendec, titles[2])
p3 <- plot_distribution_grid(samples_svd, titles[3])
p4 <- plot_distribution_grid(samples_choleski, titles[4])

fig_layout <- (p1 | p2) / (p3 | p4)
fig_layout
```

3. Random vectors generation from multivariate Gaussian distribution II: special cases. Implement random vector generators 
which generate Gaussian vectors from specific covariance structures listed below. Can you make your implementations more 
efficient than the generic generators?
    (a) Compound Symmetric Covariance $\Sigma = (\sigma_{ij})$ with $\sigma_{ii} = 1$ and $\sigma_{ij} = \rho$ when $i \neq j$ 
    where $0 \le p \le 1$.
    (b) AR(1) (Auto Regressive order 1) model $\Sigma = (\sigma_{ij})$ with $\sigma_{ij} = \rho^{|i-j|}$ where 
    $|\rho| < 1$.
    (c) Block Covariance $\Sigma = L\Omega L^{T} + D$ where $\Omega$ is a K x K positive-definite matrix, 
    $D = diag(\sigma_{1}^{2}, . . . , \sigma_{p}^{2})$, $L$ is a p x K label matrix such that each row of $L$ consists of 
    one '1' and K - 1 '0's.

```{r}
generate_compound_symmetric <- function(n, p, rho) {
  # Generate standard multivariate Gaussian random vector
  Z <- matrix(rnorm(n * p), nrow = n, ncol = p)

  # Adjust covariance matrix
  Sigma <- diag(1, p)
  Sigma[lower.tri(Sigma)] <- rho

  # Cholesky decomposition of covariance matrix
  L <- chol(Sigma)

  # Generate random vectors with the desired covariance structure
  return(Z %*% L)
}

generate_ar1 <- function(n, p, rho) {
  # Generate standard multivariate Gaussian random vector
  Z <- matrix(rnorm(n * p), nrow = n, ncol = p)

  # Create Toeplitz covariance matrix
  Sigma <- toeplitz(rho^abs(1:p - 1))

  # Cholesky decomposition of covariance matrix
  L <- chol(Sigma)

  # Generate random vectors with the desired covariance structure
  return(Z %*% L)
}

generate_block_covariance_d <- function(n, p, K, Omega, sig_sq) {
  # Generate standard multivariate Gaussian random vector
  Z <- matrix(rnorm(n * K), nrow = n, ncol = K)

  # Cholesky decomposition of Omega
  L <- chol(Omega)

  # Create label matrix L
  L <- matrix(0, nrow = p, ncol = K)
  for (i in 1:p) {
    L[i, sample(1:K, 1)] <- 1
  }

  # Perform dot product
  s_x <- Z %*% t(L) %*% L

  # Add diagonal matrix D
  for (i in 1:p) {
    s_x[, i] <- s_x[, i] + sqrt(sig_sq[i]) * rnorm(n)
  }

  # Generate random vectors with the desired covariance structure
  return(s_x)
}


# Generate random vectors for each distribution
n <- 1000
p <- 3

# Generate data for compound symmetric covariance
rho_cs <- 0.5
data_cs <- generate_compound_symmetric(n, p, rho_cs)

# Generate data for AR(1) model
rho_ar1 <- 0.7
data_ar1 <- generate_ar1(n, p, rho_ar1)

# Generate data for block covariance
K <- 3
Omega <- diag(1, K)   # Identity matrix for simplicity
sig_sq <- c(0.2)      # Define the diagonal elements of the D matrix
data_block <- generate_block_covariance_d(n, p, K, Omega, sig_sq)

# Create data frames for ggplot
df_cs <- data.frame(Value = c(data_cs),
                    Distribution = rep("Compound Symmetric", n * p))
df_ar1 <- data.frame(Value = c(data_ar1),
                     Distribution = rep("AR(1)", n * p))
df_block <- data.frame(Value = c(data_block),
                       Distribution = rep("Block Covariance", n * p))

# Combine data frames
df_combined <- rbind(df_cs, df_ar1, df_block)

# Plot distributions
ggplot(df_combined, aes(x = Value, fill = Distribution)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ Distribution, scales = "free") +
  labs(title = "Distributions of Generated Random Vectors",
       x = "Value", y = "Density") +
  theme_bw() +
  theme(legend.position = "bottom",legend.title=element_blank())

# These implementations should be more efficient than the
# generic generators by taking advantage of the specific
# covariance structures
par(mfrow = c(1, 1))
```

4. Projective center limit theorem says that the projection of a uniform distribution on the sphere in $\mathbb{R}^n$ of radius
$\sqrt{n}$ onto a line converges to the standard normal distribution as $n \rightarrow  \infty$. Show some numeric evidence of this theorem.

```{r fig.height = 3.4, fig.width = 6}
# Function to generate random points on a sphere in n-dimensional space
generate_sphere_points <- function(N, dm) {
  # Generate random spherical coordinates
  theta <- runif(N, 0, 2 * pi)
  phi <- acos(1 - 2 * runif(N))

  # Calculate Cartesian coordinates
  x <- sqrt(dm) * sin(phi) * cos(theta)
  y <- sqrt(dm) * sin(phi) * sin(theta)
  z <- sqrt(dm) * cos(phi)

  # Return matrix of points
  return(cbind(x, y, z))
}

# Number of random points on the sphere
N <- 800

# Dimensionality of the sphere
dm <- 80
# Generate random points on the sphere
sphere_points <- generate_sphere_points(N, dm)

# Choose a random direction vector (unit vector)
direction <- rnorm(dm)
direction <- direction / sqrt(sum(direction^2))

# Project points onto the line defined by the direction vector using orthogonalProjectionToLine
projected_points <- t(orthogonalProjectionToLine(t(sphere_points), direction))

par(mfrow = c(1, 2), mar = c(2, 2, 2, 2))  # Set up subplot layout
scatter3D(sphere_points[, 1], sphere_points[, 2], sphere_points[, 3],
          colvar = NULL, col = "blue", pch = 20, cex=0.05,
          xlab = "X", ylab = "Y", zlab = "Z",
          main = "3D Sphere Points")

# Plot histogram of projected points and compare with standard normal distribution
hist(projected_points, freq = FALSE, main = "Linearly Projected Sphere Points",
     xlab = "Value", ylab = "Density", breaks = 30, xlim = c(-5, 5), bty = "L")
curve(dnorm(x), add = TRUE, col = "red", lwd = 2, n = N)
par(xpd = TRUE) # turn off clipping
legend("top", legend = c("Proj. Pts.", "Std. Norm."), inset = c(1, 0),
       col = c("black", "red"), lwd = c(1, 2), xpd = TRUE,
       horiz = TRUE, bty = "n")
# par(xpd=FALSE)
par(mfrow = c(1, 1))
```

5. Marcenko-Pastur law. Let $X$ be an n x p random matrix with IID entries of mean 0 and variance
    1. Define $\hat \Sigma_{n} = \frac {1} {n} X^{T}X$ and let $\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_p$ be the eigenvalues of 
    $\hat \Sigma_{n}$. As $p,n \rightarrow \infty$ with $\frac {p} {n} \rightarrow \lambda \in (0, \infty)$, the empirical 
    distribution of the eigenvalues converges to the Marcenko-Pastur distribution with density:

    $$f(x) = \frac {1} {2\pi} \frac {\sqrt {(\lambda_+ - x)(x - \lambda_-)}} {\lambda x} 1_{x \in [\lambda_+,\lambda_-]}$$

    where $\lambda_{\pm} = (1 \pm \sqrt{\lambda})^2$. On the other hand, the classical asymptotic theory states that all 
    $\lambda_j \rightarrow 1$ as $n \rightarrow \infty$ and $p$ remains the same. Draw a histogram of the eigenvalues for a random matrix $X$ with 
    standard normal entries and $(n, p) = (1000, 500)$. Which asymptotic theory is more useful in this case? 
    (Reference: High-Dimensional Statistics by Wainwright Section 1.2.2)

```{r}
```

6. Tracy-Widom law. For an n X p random matrix X with IID standard normal entries, as both n and p tend to infinity with their 
ratio n/p converging to a constant, the asymptotic distribution of the largest eigenvalue of the sample covariance 
$\hat \Sigma_n = \frac{1} {n} X^{T}X$ is described by the Tracy-Widom law (that is quite complicated and omitted here).
    (a) Find a fast way to calculate the largest eigenvalue of a matrix.
    (b) Plot the histograms of the largest eigenvalues of 1,000 independent random matrices with IID standard normal entries 
    for $(n, p) = (100,50), (1000,500)$ and $(10000,5000)$.